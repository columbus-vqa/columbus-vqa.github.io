<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/columbus.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">COLUMBUS: Evaluating <u>CO</u>gnitive <u>L</u>ateral <u>U</u>nderstanding through <u>M</u>ultiple-choice re<u>BUS</u>es</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Koen Kraaijveld<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=npRM7lYAAAAJ&hl=en&authuser=1">Yifan Jiang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://mayer123.github.io/">Kaixin Ma</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ilievski.info/">Filip Ilievski</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Department of Computer Science, Faculty of Science, Vrije Universiteit Amsterdam</span>
            <span class="author-block"><sup>2</sup>Information Sciences Institute, University of Southern California</span><br>
            <span class="author-block"><sup>3</sup>Tencent AI Lab, Bellevue, WA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2409.04053"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2409.04053"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/koen-47/COLUMBUS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/koen-47/COLUMBUS/tree/main/notebooks"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="static/images/jupyter-icon.svg"/>
                  </span>
                  <span>Notebooks</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/koen-47/COLUMBUS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/vertical_vs_lateral_thinking_example_v2.png">
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While visual question-answering (VQA) benchmarks have catalyzed the development of reasoning techniques, they have focused on vertical thinking. Effective problem-solving also necessitates lateral thinking, which remains understudied in AI and has not been used to test visual perception systems. To bridge this gap, we formulate visual lateral thinking as a multiple-choice question-answering task and describe a three-step taxonomy-driven methodology for instantiating task examples. Then, we develop COLUMBUS, a synthetic benchmark that applies the task pipeline to create QA sets with text and icon rebus puzzles based on publicly available collections of compounds and common phrases. COLUMBUS comprises over 1,000 puzzles, each with four answer candidates. While the SotA vision language models (VLMs) achieve decent performance, our evaluation demonstrates a substantial gap between humans and models. VLMs benefit from human-curated descriptions but struggle to self-generate such representations at the right level of abstraction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="static/images/puzzle_examples/arrow_in_the_quiver_icon.png"/><br>
          <span>A) Arrowheads</span><br>
          <span>B) Draw the long bow</span><br>
          <span><b><u>C) Arrow in the quiver</u></b></span><br>
          <span>D) Make one's bow</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/back_the_wrong_horse_icon.png"/><br>
          <span><b><u>A) Back the wrong horse</u></b></span><br>
          <span>B) Dark horse</span><br>
          <span>C) High horse</span><br>
          <span>D) Horse around</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/before_one's_eyes_2_icon.png"/><br>
          <span>A) Under one's very eyes</span><br>
          <span><b><u>B) Before one's eyes</u></b></span><br>
          <span>C) Before one's time</span><br>
          <span>D) Before someone's time</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/roll_out_the_red_carpet.png"/><br>
          <span>A) Red carpet treatment</span><br>
          <span>B) Under the carpet</span><br>
          <span><b><u>C) Roll out the red carpet</u></b></span><br>
          <span>D) Roll in the aisles</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/cross_fingers_for_luck_1_icon.png"/><br>
          <span>A) Stroke of luck</span><br>
          <span><b><u>B) Cross fingers for luck</u></b></span><br>
          <span>C) Push one's luck</span><br>
          <span>D) Down on one's luck</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/dig_in_one's_heels_icon.png"/><br>
          <span>A) Dig one's heels in</span><br>
          <span>B) Take to one's heels</span><br>
          <span><b><u>C) Dig in one's heels</u></b></span><br>
          <span>D) Dig heels in</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/down_on_one's_luck_icon.png"/><br>
          <span>A) Ride one's luck</span><br>
          <span>B) Cross fingers for luck</span><br>
          <span><b><u>C) Down on one's luck</u></b></span><br>
          <span>D) Push one's luck</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/go_down_in_flames_1.png"/><br>
          <span>A) Go down on</span><br>
          <span>B) Go up in flames</span><br>
          <span><b><u>C) Go down in flames</u></b></span><br>
          <span>D) Go up in smoke</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/handwriting_on_the_wall_icon.png"/><br>
          <span>A) Writing on the wall</span><br>
          <span><b><u>B) Handwriting on the wall</u></b></span><br>
          <span>C) Handwriting</span><br>
          <span>D) Pen picture</span><br>
          <span>&#8192;</span>
        </div>
        <div class="item">
          <img src="static/images/puzzle_examples/hang_up_one's_hat_1_icon.png"/><br>
          <span><b><u>A) Hang up one's hat</u></b></span><br>
          <span>B) Hang one's hat</span><br>
          <span>C) Hang over one's head</span><br>
          <span>D) Tip one's hat</span><br>
          <span>&#8192;</span>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Introducing Visual Lateral Tasks and COLUMBUS</h2>

    <!-- Taxonomy. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <p>
          A visual lateral thinking task is driven by a taxonomy consisting of 18 rules organized across three categories. 
          Each rule uniquely manipulates the appearance and visual-spatial relationships of each element (either text or an icon) in a puzzle.
          The three categories are as follows:
          
          <br>
          1. <b>Individual</b> rules define the
          unary characteristics of an element in a rebus. Example rules
          include reversing character order (direction), the text color
          (style), and adding arrows before the element (highlight).
          
          <br>
          2. <b>Relational</b> rules define the positioning between a pair
          of elements. We define four relational rules, placing an element beside/inside/above/outside another.
          
          <br>
          3. <b>Modifier</b> rules are designed to be mutually inclusive with other individual rules. 
          Examples include repeating an element multiple times
          or substituting it with a phonetically similar element.
          <br><br>
        </p>

        <img src="static/images/rule_taxonomy.png">
      </div>
      <br/>
    </div>


    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Our Approach</h2>
          <p class="paragraph">
            The pipeline to instantiate and evaluate visual lateral thinking tasks is shown below. 
            Puzzle generation leverages the above taxonomy to create a graph representation
            for a puzzle answer and generate an image for the graph. 
            
            Each graph is a directed, 
            attributed graph whose nodes are elements that will be rendered into a puzzle image.
            The node attributes specify the
            rendering of that element (i.e., the individual or modifier
            rules that will apply to it). The edges between two nodes
            are annotated with an attribute that specifies their relational
            rule. 
            The distractor sampling step is based on a weighted average 
            of orthographic and semantic similarity between a
            puzzle’s correct answer and its visible elements.
          </p>
          
          <img src="static/images/pipeline_overview.png">
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-4">The COLUMBUS Benchmark</h2>
        <p class="paragraph">
          COLUMBUS is generated from collections of compound words and common phrases that were either web scraped, downloaded or manually added.
          The benchmark comprises over 1,000 puzzles spread over two partitions: 
          <span class="textsc">COLUMBUS-text</span>, with puzzles that only contain text and <span class="textsc">COLUMBUS-icon</span>, with puzzles that contain at least one icon.
        </p>
        <br>
        <div class="columns is-centered">
          <img src="static/images/key_statistics_table.png">
        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <h2 class="title is-3">Results</h2>
    <!-- <br><br> -->
    <div class="columns is-centered">
      <div class="column">
        <h2 class="title is-4">Overall Performance</h2>
        <p class="paragraph">
          The models we test include open- and closed-source
          instruction-tuned and non-instruction-tuned vision-language
          models. We also experiment with two structural variants of closed-
          source models: forward (FC) and backward chaining (BC).
          All models are evaluated in a zero-shot setting using standard hyperparameter values.
          <br><br>
          Generally, the closed-source and larger open-source models perform best on both partitions, while the small, 
          non-instruction-tuned models perform near-randomly and have strong biases to certain answers (typically A or D).
          The best model for each partition
          is consistently GPT-4o, which is the expected result. Yet,
          none of the models surpass human accuracy, with average
          gaps of 45.15% on <span class="textsc">COLUMBUS-text</span> and 35.75% on
          <span class="textsc">COLUMBUS-icon</span>.  
          <br><br>
          Forward chaining yields marginal improvement on GPT-4o(-mini) while boosting Gemini performance greatly (6% to 17%). 
          On the contrary, backward chaining yields an 18.3% and 25.15% drop in accuracy for GPT-4o and GPT-4o-mini,
          averaged across the two partitions.
        </p>
      </div>
      <div class="column">
        <div class="content">
          <img src="static/images/main_results_table.png" width="90%" height="90%">
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column">
        <h2 class="title is-4">Model Sensitivity to Input Information</h2>
        <p class="paragraph">         
          Can models benefit from a ground-truth structured description of the puzzle provided in their input?
          We experiment with four prompts, each of which supplies a model with varying degrees of additional information
          that can be used to solve a puzzle (1 = least information, 4 = most information). 
          For both <span class="textsc">COLUMBUS-text</span> (left plot)
          <span class="textsc">COLUMBUS-icon</span> (right plot), we generally see that adding increasingly more information results in better performance.
        </p>
          <img src="static/images/model_sensitivity_graphs.png">
      </div>
      <div class="column">
        <h2 class="title is-4">VLM Generation of Puzzles</h2>
        <p class="paragraph">
          Given the strong generative abilities of VLMs, a natural question arises: can they generate puzzles without our
          methodology? Through a user study, we find that humans overwhelmingly prefer our puzzles over
          ones generated by DALLE-3. An example is shown below with a puzzle from COLUMBUS (left) and a puzzle generated by DALLE-3 (right). 
        </p>
        <br>
        <div class="columns is-centered">
          <img src="static/images/rebus_generation_images.png" width="80%" height="80%">
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Rule-based Analysis</h2>
          <p class="paragraph">
            We also perform an analysis of puzzles solved per rule using GPT-4o. We find that performance is much better on <i>relational</i> rather
            <i>individual</i> rules, and slightly better when <i>individual</i> rules are combined with <i>modifier</i> rules. We also note that, 
            while the GPT-4o’s performance is similar on the two partitions, specific rules are more difficult for this model when represented 
            as text (e.g., repetition rules). In contrast, others are more challenging when presented as icons (e.g., size).
          </p>
          <img src="static/images/gpt_rule_analysis_graph.png">
        </div>
      </div>
    </div>

    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation (BibTeX)</h2>
    If you have found COLUMBUS useful, please cite us: <br><br>
    <pre><code>@article{kraaijveld2024columbus,
      title={COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes}, 
      author={Koen Kraaijveld and Yifan Jiang and Kaixin Ma and Filip Ilievski},
      year={2024},
      eprint={2409.04053},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.04053}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
